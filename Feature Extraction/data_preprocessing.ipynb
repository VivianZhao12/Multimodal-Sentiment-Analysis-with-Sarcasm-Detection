{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-2LTb3Olc7CY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xJmdSAsmc81_"
   },
   "outputs": [],
   "source": [
    "audio_feature = pd.read_csv(\"/Users/amily/Desktop/audio_feature.csv\")\n",
    "text_feature = pd.read_csv('/Users/amily/Desktop/text_feature.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KS_o1IICy6Al"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_string_to_list(vector_string):\n",
    "    \"\"\"\n",
    "    Convert a space-separated string of numbers into a list of floats.\n",
    "\n",
    "    Args:\n",
    "    - vector_string (str): String representation of a vector.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of floats extracted from the input string.\n",
    "    \"\"\"\n",
    "    # Remove brackets and split by spaces\n",
    "    clean_string = vector_string.strip(\"[]\")  # Remove leading and trailing brackets\n",
    "    string_values = clean_string.split()      # Split by spaces\n",
    "\n",
    "    # Convert to a list of floats\n",
    "    float_values = [float(value) for value in string_values]\n",
    "    return float_values\n",
    "#merged_df[\"word2vec_pretrained_with_stop\"] = merged_df[\"word2vec_pretrained_with_stop\"].apply(convert_string_to_list)\n",
    "#merged_df[\"word2vec_pretrained_without_stop\"] = merged_df[\"word2vec_pretrained_without_stop\"].apply(convert_string_to_list)\n",
    "#merged_df['mfccs'] = merged_df['mfccs'].apply(ast.literal_eval)\n",
    "\n",
    "audio_feature['mfccs'] = audio_feature['mfccs'].apply(ast.literal_eval)\n",
    "text_feature[\"word2vec_pretrained_with_stop\"] = text_feature[\"word2vec_pretrained_with_stop\"].apply(convert_string_to_list)\n",
    "text_feature[\"word2vec_pretrained_without_stop\"] = text_feature[\"word2vec_pretrained_without_stop\"].apply(convert_string_to_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "raY53-R843bP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def expand_list_column(df, list_column, prefix):\n",
    "    \"\"\"\n",
    "    Expands a column of lists into multiple columns.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame containing the list column.\n",
    "    - list_column (str): The name of the column with lists to expand.\n",
    "    - prefix (str): Prefix for the new columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with the list column expanded into separate columns.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the list column where each list element becomes a new column\n",
    "    expanded_cols = pd.DataFrame(df[list_column].tolist(), index=df.index)\n",
    "\n",
    "    # Rename the new columns with the specified prefix\n",
    "    expanded_cols = expanded_cols.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "    # Drop the original list column and join the expanded columns\n",
    "    df = df.drop(columns=[list_column]).join(expanded_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "#merged_df = expand_list_column(merged_df, list_column=\"word2vec_pretrained_with_stop\", prefix=\"wpwiths\")\n",
    "#merged_df = expand_list_column(merged_df, list_column=\"word2vec_pretrained_without_stop\", prefix=\"wpwithouts\")\n",
    "#merged_df = expand_list_column(merged_df, list_column=\"mfccs\", prefix=\"mfccs\")\n",
    "\n",
    "text_feature  = expand_list_column(text_feature , list_column=\"word2vec_pretrained_with_stop\", prefix=\"wpwiths\")\n",
    "text_feature = expand_list_column(text_feature, list_column=\"word2vec_pretrained_without_stop\", prefix=\"wpwithouts\")\n",
    "audio_feature = expand_list_column(audio_feature, list_column=\"mfccs\", prefix=\"mfccs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAOntDtr8S7h",
    "outputId": "e7ebb322-d647-41a7-c234-3886db5796d8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle as pk\n",
    "\n",
    "columns_to_exclude = [\"file_id\",\"duration\",'Unnamed: 0', 'sarcasm', 'text', 'label', 'emotion',\n",
    "       'tokenized_without_stop', 'tokenized_with_stop', 'sentence_split','sarcasm_label', 'emotion_label', 'sentiment_label', 'id', 'spectral_centroid', 'spectral_bandwidth', 'pitch', 'energy',\n",
    "       'loudness', 'sentence_level_similarity_emotion',\n",
    "       'sentence_level_similarity_word', 'exclamation']\n",
    "columns_to_pca_text = [i for i in text_feature.columns if i not in columns_to_exclude]\n",
    "columns_to_pca_audio = [i for i in audio_feature.columns if i not in columns_to_exclude]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "text_subset_data = scaler.fit_transform(text_feature[columns_to_pca_text])\n",
    "text_pca = PCA()\n",
    "text_result = text_pca.fit_transform(text_subset_data)\n",
    "pk.dump(text_pca, open(\"pca_text.pkl\", \"wb\"))\n",
    "\n",
    "audio_subset_data = scaler.fit_transform(audio_feature[columns_to_pca_audio])\n",
    "audio_pca = PCA()\n",
    "audio_result = audio_pca.fit_transform(audio_subset_data)\n",
    "pk.dump(audio_pca, open(\"pca_audio.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "def apply_pca_to_subset(df, subset_columns, n_components, pca_name, prefix=\"PCA\"):\n",
    "    \"\"\"\n",
    "    Apply PCA to a subset of columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - subset_columns (list): List of column names to apply PCA on.\n",
    "    - n_components (int): Number of principal components to keep.\n",
    "    - prefix (str): Prefix for the new PCA columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Updated DataFrame with PCA-transformed columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Standardize the selected columns\n",
    "    scaler = StandardScaler()\n",
    "    subset_data = scaler.fit_transform(df[subset_columns])\n",
    "    \n",
    "    pca_reload = pk.load(open(pca_name,\"rb\"))\n",
    "    pca_components = pca_reload.transform(subset_data)\n",
    "    pca_components = pca_components[:, :n_components]\n",
    "\n",
    "    pca_columns = [f\"{prefix}_{i+1}\" for i in range(n_components)]\n",
    "    pca_df = pd.DataFrame(pca_components, columns=pca_columns, index=df.index)\n",
    "\n",
    "    df = df.drop(columns=subset_columns)  # Drop the original columns\n",
    "    df = pd.concat([df, pca_df], axis=1)  # Add PCA components\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "text_feature = apply_pca_to_subset(text_feature, columns_to_pca_text, 50,\"pca_text.pkl\", prefix=\"TEXT\")\n",
    "audio_feature = apply_pca_to_subset(audio_feature, columns_to_pca_audio, 8, \"pca_audio.pkl\",prefix=\"AUDIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(audio_feature, text_feature, on=\"id\", how=\"inner\")\n",
    "columns_to_exclude = ['file_id','text_x','sarcasm_y', 'text_y', 'label_y', 'emotion_y','tokenized_without_stop',\n",
    "       'tokenized_with_stop', 'sentence_split']\n",
    "merged_remain = [i for i in merged_df .columns if i not in columns_to_exclude]\n",
    "merged_df = merged_df[merged_remain]\n",
    "merged_df['exclamation'] = merged_df['exclamation'] .astype(int)\n",
    "merged_df.rename(columns={'sarcasm_x': 'sarcasm_label', 'emotion_x': 'emotion_label',\"label_x\": \"sentiment_label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYJcDfxe8cZe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "3R-V33U85GDF"
   },
   "outputs": [],
   "source": [
    "sarcasm_only_df = merged_df.loc[merged_df['emotion_label'] == 'sarcasm'][\"id\"]\n",
    "anger_only_df = merged_df.loc[merged_df['emotion_label'] == 'anger'][\"id\"]\n",
    "disgust_only_df = merged_df.loc[merged_df['emotion_label'] == 'disgust'][\"id\"]\n",
    "fear_only_df = merged_df.loc[merged_df['emotion_label'] == 'fear'][\"id\"]\n",
    "joy_only_df = merged_df.loc[merged_df['emotion_label'] == 'joy'][\"id\"]\n",
    "sadness_only_df = merged_df.loc[merged_df['emotion_label'] == 'sadness'][\"id\"]\n",
    "surprise_only_df = merged_df.loc[merged_df['emotion_label'] == 'surprise'][\"id\"]\n",
    "non_sarcasm_negative_only_df = merged_df.loc[(merged_df['emotion_label'] != 'sarcasm') & (merged_df['sentiment_label'] == 0), \"id\"]\n",
    "positive_only_df = merged_df.loc[(merged_df['sentiment_label'] == 1), \"id\"]\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "sarcasm_gerneral_negative = list(sarcasm_only_df.sample(n=30,random_state = 42))\n",
    "rest_negative = list(non_sarcasm_negative_only_df.sample(n=70, random_state = 42))\n",
    "sentiment_index = list(positive_only_df) + rest_negative + sarcasm_gerneral_negative\n",
    "\n",
    "rest = list(merged_df.loc[~merged_df['id'].isin(sentiment_index),\"id\"])\n",
    "\n",
    "sentiment = merged_df[merged_df['id'].isin(sentiment_index)]\n",
    "sarcasm = merged_df[~merged_df['id'].isin(sentiment_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "3Nx0J1NafARs"
   },
   "outputs": [],
   "source": [
    "sarcasm.to_csv(\"/Users/amily/Desktop/sarcasm_dataset.csv\")\n",
    "sentiment.to_csv(\"/Users/amily/Desktop/sentiment_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = [\"sarcasm_label\",\"emotion_label\", \"sentiment_label\",\"id\",\"spectral_centroid\",\"spectral_bandwidth\",\"pitch\",\"energy\",\"loudness\"]\n",
    "text_features = [\"sarcasm_label\",\"emotion_label\",\"sentiment_label\",\"id\",\"sentence_level_similarity_emotion\",\"sentence_level_similarity_word\",\"exclamation\"]\n",
    "\n",
    "#audio feature filter\n",
    "text = text_features + [col for col in sarcasm.columns if ('text' in col.lower())]\n",
    "#text feature filter\n",
    "audio = text_features + [col for col in sarcasm.columns if ('audio' in col.lower())]\n",
    "\n",
    "sentiment_text = sentiment[text]\n",
    "sentiment_audio = sentiment[audio]\n",
    "sarcasm_text = sarcasm[text]\n",
    "sarcasm_audio = sarcasm[audio]\n",
    "\n",
    "sentiment_text.to_csv(\"/Users/amily/Desktop/text_sentiment_dataset.csv\")\n",
    "sentiment_audio.to_csv(\"/Users/amily/Desktop/audio_sentiment_dataset.csv\")\n",
    "sarcasm_text.to_csv(\"/Users/amily/Desktop/text_sarcasm_dataset.csv\")\n",
    "sarcasm_audio.to_csv(\"/Users/amily/Desktop/audio_sarcasm_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sarcasm_label</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_level_similarity_emotion</th>\n",
       "      <th>sentence_level_similarity_word</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>TEXT_1</th>\n",
       "      <th>TEXT_2</th>\n",
       "      <th>TEXT_3</th>\n",
       "      <th>...</th>\n",
       "      <th>TEXT_41</th>\n",
       "      <th>TEXT_42</th>\n",
       "      <th>TEXT_43</th>\n",
       "      <th>TEXT_44</th>\n",
       "      <th>TEXT_45</th>\n",
       "      <th>TEXT_46</th>\n",
       "      <th>TEXT_47</th>\n",
       "      <th>TEXT_48</th>\n",
       "      <th>TEXT_49</th>\n",
       "      <th>TEXT_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.560276</td>\n",
       "      <td>9.361766</td>\n",
       "      <td>-1.944473</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.528326</td>\n",
       "      <td>-5.195582</td>\n",
       "      <td>-2.071413</td>\n",
       "      <td>2.389241</td>\n",
       "      <td>0.679799</td>\n",
       "      <td>1.464318</td>\n",
       "      <td>2.688781</td>\n",
       "      <td>1.950308</td>\n",
       "      <td>-1.281537</td>\n",
       "      <td>-0.505422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.882026</td>\n",
       "      <td>-10.278302</td>\n",
       "      <td>1.993432</td>\n",
       "      <td>...</td>\n",
       "      <td>4.879186</td>\n",
       "      <td>1.806766</td>\n",
       "      <td>-7.082715</td>\n",
       "      <td>-2.375127</td>\n",
       "      <td>-2.932272</td>\n",
       "      <td>1.903256</td>\n",
       "      <td>7.856745</td>\n",
       "      <td>4.622939</td>\n",
       "      <td>1.243021</td>\n",
       "      <td>2.524237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>0.913640</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.933006</td>\n",
       "      <td>-1.102636</td>\n",
       "      <td>-2.704832</td>\n",
       "      <td>...</td>\n",
       "      <td>8.493502</td>\n",
       "      <td>3.512994</td>\n",
       "      <td>2.752871</td>\n",
       "      <td>-1.689377</td>\n",
       "      <td>-3.118805</td>\n",
       "      <td>0.926467</td>\n",
       "      <td>-0.505093</td>\n",
       "      <td>0.921205</td>\n",
       "      <td>-2.015440</td>\n",
       "      <td>0.257554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009295</td>\n",
       "      <td>0.920026</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.994384</td>\n",
       "      <td>1.081702</td>\n",
       "      <td>2.105956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143942</td>\n",
       "      <td>-1.922510</td>\n",
       "      <td>1.761534</td>\n",
       "      <td>-0.180522</td>\n",
       "      <td>-0.999845</td>\n",
       "      <td>-4.713001</td>\n",
       "      <td>0.863674</td>\n",
       "      <td>-0.488971</td>\n",
       "      <td>-1.116376</td>\n",
       "      <td>-2.301616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.942157</td>\n",
       "      <td>-0.354175</td>\n",
       "      <td>-1.530681</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.793798</td>\n",
       "      <td>-1.769770</td>\n",
       "      <td>0.242620</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>-1.445818</td>\n",
       "      <td>-2.013260</td>\n",
       "      <td>0.900879</td>\n",
       "      <td>1.465607</td>\n",
       "      <td>0.475343</td>\n",
       "      <td>-2.228691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.692437</td>\n",
       "      <td>-2.622186</td>\n",
       "      <td>-2.577338</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.737109</td>\n",
       "      <td>-4.056155</td>\n",
       "      <td>0.947543</td>\n",
       "      <td>1.190202</td>\n",
       "      <td>-1.446018</td>\n",
       "      <td>-2.297449</td>\n",
       "      <td>-1.035243</td>\n",
       "      <td>-0.385022</td>\n",
       "      <td>2.778561</td>\n",
       "      <td>1.791775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "      <td>-0.688249</td>\n",
       "      <td>0.938817</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.782721</td>\n",
       "      <td>-6.644730</td>\n",
       "      <td>2.585404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281552</td>\n",
       "      <td>0.693386</td>\n",
       "      <td>-0.137404</td>\n",
       "      <td>-0.627561</td>\n",
       "      <td>-0.753154</td>\n",
       "      <td>1.652641</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>3.201987</td>\n",
       "      <td>1.966591</td>\n",
       "      <td>3.279186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.147711</td>\n",
       "      <td>12.323190</td>\n",
       "      <td>9.173530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133216</td>\n",
       "      <td>0.857063</td>\n",
       "      <td>-0.324311</td>\n",
       "      <td>1.159971</td>\n",
       "      <td>1.459785</td>\n",
       "      <td>-0.298021</td>\n",
       "      <td>0.313660</td>\n",
       "      <td>4.078432</td>\n",
       "      <td>1.701545</td>\n",
       "      <td>-2.340982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>0.779693</td>\n",
       "      <td>0.706350</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.075184</td>\n",
       "      <td>-1.541483</td>\n",
       "      <td>1.071947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924546</td>\n",
       "      <td>-1.730823</td>\n",
       "      <td>0.306840</td>\n",
       "      <td>1.716419</td>\n",
       "      <td>-0.743047</td>\n",
       "      <td>2.220859</td>\n",
       "      <td>-2.712793</td>\n",
       "      <td>-0.492945</td>\n",
       "      <td>2.053580</td>\n",
       "      <td>-0.552733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.679678</td>\n",
       "      <td>-1.659085</td>\n",
       "      <td>-5.376955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490707</td>\n",
       "      <td>-0.570019</td>\n",
       "      <td>-2.119824</td>\n",
       "      <td>-0.758980</td>\n",
       "      <td>0.836084</td>\n",
       "      <td>0.162701</td>\n",
       "      <td>1.185396</td>\n",
       "      <td>-0.860950</td>\n",
       "      <td>-0.592937</td>\n",
       "      <td>1.503745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sarcasm_label emotion_label  sentiment_label   id  \\\n",
       "0                0         anger                0    1   \n",
       "1                0         anger                0    2   \n",
       "2                0         anger                0    3   \n",
       "3                0         anger                0    4   \n",
       "4                0         anger                0    5   \n",
       "..             ...           ...              ...  ...   \n",
       "419              0      surprise                0  420   \n",
       "421              0      surprise                0  422   \n",
       "422              0      surprise                0  423   \n",
       "425              0      surprise                0  426   \n",
       "427              0      surprise                0  428   \n",
       "\n",
       "     sentence_level_similarity_emotion  sentence_level_similarity_word  \\\n",
       "0                             0.000000                        0.000000   \n",
       "1                             0.000000                        0.000000   \n",
       "2                             0.439865                        0.913640   \n",
       "3                             0.009295                        0.920026   \n",
       "4                             0.000000                        0.000000   \n",
       "..                                 ...                             ...   \n",
       "419                           0.000000                        0.000000   \n",
       "421                          -0.688249                        0.938817   \n",
       "422                           0.000000                        0.000000   \n",
       "425                           0.779693                        0.706350   \n",
       "427                           0.000000                        0.000000   \n",
       "\n",
       "     exclamation    TEXT_1     TEXT_2    TEXT_3  ...   TEXT_41   TEXT_42  \\\n",
       "0              0  4.560276   9.361766 -1.944473  ... -3.528326 -5.195582   \n",
       "1              0 -1.882026 -10.278302  1.993432  ...  4.879186  1.806766   \n",
       "2              0 -3.933006  -1.102636 -2.704832  ...  8.493502  3.512994   \n",
       "3              1 -0.994384   1.081702  2.105956  ...  0.143942 -1.922510   \n",
       "4              1 -3.942157  -0.354175 -1.530681  ... -1.793798 -1.769770   \n",
       "..           ...       ...        ...       ...  ...       ...       ...   \n",
       "419            0 -6.692437  -2.622186 -2.577338  ... -1.737109 -4.056155   \n",
       "421            0 -4.782721  -6.644730  2.585404  ... -0.281552  0.693386   \n",
       "422            1 -1.147711  12.323190  9.173530  ...  0.133216  0.857063   \n",
       "425            1 -1.075184  -1.541483  1.071947  ...  0.924546 -1.730823   \n",
       "427            0 -5.679678  -1.659085 -5.376955  ... -0.490707 -0.570019   \n",
       "\n",
       "      TEXT_43   TEXT_44   TEXT_45   TEXT_46   TEXT_47   TEXT_48   TEXT_49  \\\n",
       "0   -2.071413  2.389241  0.679799  1.464318  2.688781  1.950308 -1.281537   \n",
       "1   -7.082715 -2.375127 -2.932272  1.903256  7.856745  4.622939  1.243021   \n",
       "2    2.752871 -1.689377 -3.118805  0.926467 -0.505093  0.921205 -2.015440   \n",
       "3    1.761534 -0.180522 -0.999845 -4.713001  0.863674 -0.488971 -1.116376   \n",
       "4    0.242620  0.510232 -1.445818 -2.013260  0.900879  1.465607  0.475343   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "419  0.947543  1.190202 -1.446018 -2.297449 -1.035243 -0.385022  2.778561   \n",
       "421 -0.137404 -0.627561 -0.753154  1.652641  0.852713  3.201987  1.966591   \n",
       "422 -0.324311  1.159971  1.459785 -0.298021  0.313660  4.078432  1.701545   \n",
       "425  0.306840  1.716419 -0.743047  2.220859 -2.712793 -0.492945  2.053580   \n",
       "427 -2.119824 -0.758980  0.836084  0.162701  1.185396 -0.860950 -0.592937   \n",
       "\n",
       "      TEXT_50  \n",
       "0   -0.505422  \n",
       "1    2.524237  \n",
       "2    0.257554  \n",
       "3   -2.301616  \n",
       "4   -2.228691  \n",
       "..        ...  \n",
       "419  1.791775  \n",
       "421  3.279186  \n",
       "422 -2.340982  \n",
       "425 -0.552733  \n",
       "427  1.503745  \n",
       "\n",
       "[257 rows x 57 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_text"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
