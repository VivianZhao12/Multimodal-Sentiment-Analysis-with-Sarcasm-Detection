{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "-2LTb3Olc7CY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_feature = pd.read_csv(\"/content/audio_feature.csv\")\n",
        "text_feature = pd.read_csv('/content/text_feature.csv')\n"
      ],
      "metadata": {
        "id": "xJmdSAsmc81_"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(audio_feature, text_feature, on=\"id\", how=\"inner\")\n",
        "merged_df = merged_df[[\"sarcasm_x\",\"emotion_x\",\"label_x\",'id',\"mfccs\",\"spectral_centroid\",\"spectral_bandwidth\",\"pitch\",\"energy\",\"loudness\",'sentence_level_similarity_emotion',\"sentence_level_similarity_word\",\"exclamation\",'word2vec_pretrained_with_stop', 'word2vec_pretrained_without_stop']]"
      ],
      "metadata": {
        "id": "l778EWBgnl0b"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.rename(columns={'sarcasm_x': 'sarcasm_label', 'emotion_x': 'emotion_label',\"label_x\": \"sentiment_label\"}, inplace=True)"
      ],
      "metadata": {
        "id": "s8FRpqP0fqHw"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['exclamation'] = merged_df['exclamation'] .astype(int)"
      ],
      "metadata": {
        "id": "kotE3aEezBkG"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "def convert_string_to_list(vector_string):\n",
        "    \"\"\"\n",
        "    Convert a space-separated string of numbers into a list of floats.\n",
        "\n",
        "    Args:\n",
        "    - vector_string (str): String representation of a vector.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of floats extracted from the input string.\n",
        "    \"\"\"\n",
        "    # Remove brackets and split by spaces\n",
        "    clean_string = vector_string.strip(\"[]\")  # Remove leading and trailing brackets\n",
        "    string_values = clean_string.split()      # Split by spaces\n",
        "\n",
        "    # Convert to a list of floats\n",
        "    float_values = [float(value) for value in string_values]\n",
        "    return float_values\n",
        "merged_df[\"word2vec_pretrained_with_stop\"] = merged_df[\"word2vec_pretrained_with_stop\"].apply(convert_string_to_list)\n",
        "merged_df[\"word2vec_pretrained_without_stop\"] = merged_df[\"word2vec_pretrained_without_stop\"].apply(convert_string_to_list)\n",
        "merged_df['mfccs'] = merged_df['mfccs'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "KS_o1IICy6Al"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def expand_list_column(df, list_column, prefix):\n",
        "    \"\"\"\n",
        "    Expands a column of lists into multiple columns.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): The DataFrame containing the list column.\n",
        "    - list_column (str): The name of the column with lists to expand.\n",
        "    - prefix (str): Prefix for the new columns.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with the list column expanded into separate columns.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame from the list column where each list element becomes a new column\n",
        "    expanded_cols = pd.DataFrame(df[list_column].tolist(), index=df.index)\n",
        "\n",
        "    # Rename the new columns with the specified prefix\n",
        "    expanded_cols = expanded_cols.add_prefix(f\"{prefix}_\")\n",
        "\n",
        "    # Drop the original list column and join the expanded columns\n",
        "    df = df.drop(columns=[list_column]).join(expanded_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "merged_df = expand_list_column(merged_df, list_column=\"word2vec_pretrained_with_stop\", prefix=\"wpwiths\")\n",
        "merged_df = expand_list_column(merged_df, list_column=\"word2vec_pretrained_without_stop\", prefix=\"wpwithouts\")\n",
        "merged_df = expand_list_column(merged_df, list_column=\"mfccs\", prefix=\"mfccs\")\n"
      ],
      "metadata": {
        "id": "raY53-R843bP"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def apply_pca_to_subset(df, subset_columns, n_components, prefix=\"PCA\"):\n",
        "    \"\"\"\n",
        "    Apply PCA to a subset of columns in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): The input DataFrame.\n",
        "    - subset_columns (list): List of column names to apply PCA on.\n",
        "    - n_components (int): Number of principal components to keep.\n",
        "    - prefix (str): Prefix for the new PCA columns.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Updated DataFrame with PCA-transformed columns.\n",
        "    \"\"\"\n",
        "    # Step 1: Standardize the selected columns\n",
        "    scaler = StandardScaler()\n",
        "    subset_data = scaler.fit_transform(df[subset_columns])\n",
        "\n",
        "    # Step 2: Apply PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_components = pca.fit_transform(subset_data)\n",
        "\n",
        "    # Step 3: Create a DataFrame for PCA components\n",
        "    pca_columns = [f\"{prefix}_{i+1}\" for i in range(n_components)]\n",
        "    pca_df = pd.DataFrame(pca_components, columns=pca_columns, index=df.index)\n",
        "\n",
        "    # Step 4: Integrate PCA results back into the original DataFrame\n",
        "    df = df.drop(columns=subset_columns)  # Drop the original columns\n",
        "    df = pd.concat([df, pca_df], axis=1)  # Add PCA components\n",
        "\n",
        "    # Optional: Print explained variance\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    print(\"Explained variance ratio of each component:\", explained_variance)\n",
        "    print(\"Total explained variance:\", explained_variance.sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "columns_to_exclude = ['sarcasm_label', 'emotion_label', 'sentiment_label', 'id', 'spectral_centroid', 'spectral_bandwidth', 'pitch', 'energy',\n",
        "       'loudness', 'sentence_level_similarity_emotion',\n",
        "       'sentence_level_similarity_word', 'exclamation']\n",
        "columns_to_pca = [i for i in merged_df.columns if i not in columns_to_exclude]\n",
        "merged_df = apply_pca_to_subset(merged_df, columns_to_pca, 50, prefix=\"PCA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAOntDtr8S7h",
        "outputId": "e7ebb322-d647-41a7-c234-3886db5796d8"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each component: [0.08396164 0.07266012 0.04122102 0.03649229 0.03062402 0.02862101\n",
            " 0.02601573 0.02372405 0.02173488 0.02049162 0.01924283 0.01896717\n",
            " 0.01804777 0.0153255  0.01446341 0.01415297 0.01325934 0.01263183\n",
            " 0.0120798  0.01159052 0.01124044 0.01084427 0.01011255 0.00941507\n",
            " 0.00932782 0.00906426 0.00893638 0.00851068 0.00824834 0.0079896\n",
            " 0.00792479 0.00754995 0.00736057 0.0071931  0.00700132 0.00673574\n",
            " 0.00641341 0.00628771 0.00595    0.005773   0.00562909 0.00557653\n",
            " 0.00542661 0.00535362 0.00519214 0.005002   0.00474166 0.00466856\n",
            " 0.00457231 0.00447798]\n",
            "Total explained variance: 0.7478270227362694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Thm1Tv9iHK",
        "outputId": "daca71cf-57f2-4e05-d1a3-d08d551585c9"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sarcasm_label', 'emotion_label', 'sentiment_label', 'id',\n",
              "       'spectral_centroid', 'spectral_bandwidth', 'pitch', 'energy',\n",
              "       'loudness', 'sentence_level_similarity_emotion',\n",
              "       'sentence_level_similarity_word', 'exclamation', 'PCA_1', 'PCA_2',\n",
              "       'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10',\n",
              "       'PCA_11', 'PCA_12', 'PCA_13', 'PCA_14', 'PCA_15', 'PCA_16', 'PCA_17',\n",
              "       'PCA_18', 'PCA_19', 'PCA_20', 'PCA_21', 'PCA_22', 'PCA_23', 'PCA_24',\n",
              "       'PCA_25', 'PCA_26', 'PCA_27', 'PCA_28', 'PCA_29', 'PCA_30', 'PCA_31',\n",
              "       'PCA_32', 'PCA_33', 'PCA_34', 'PCA_35', 'PCA_36', 'PCA_37', 'PCA_38',\n",
              "       'PCA_39', 'PCA_40', 'PCA_41', 'PCA_42', 'PCA_43', 'PCA_44', 'PCA_45',\n",
              "       'PCA_46', 'PCA_47', 'PCA_48', 'PCA_49', 'PCA_50'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zYJcDfxe8cZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_only_df = merged_df.loc[merged_df['emotion_label'] == 'sarcasm'][\"id\"]\n",
        "anger_only_df = merged_df.loc[merged_df['emotion_label'] == 'anger'][\"id\"]\n",
        "disgust_only_df = merged_df.loc[merged_df['emotion_label'] == 'disgust'][\"id\"]\n",
        "fear_only_df = merged_df.loc[merged_df['emotion_label'] == 'fear'][\"id\"]\n",
        "joy_only_df = merged_df.loc[merged_df['emotion_label'] == 'joy'][\"id\"]\n",
        "sadness_only_df = merged_df.loc[merged_df['emotion_label'] == 'sadness'][\"id\"]\n",
        "surprise_only_df = merged_df.loc[merged_df['emotion_label'] == 'surprise'][\"id\"]\n",
        "non_sarcasm_negative_only_df = merged_df.loc[(merged_df['emotion_label'] != 'sarcasm') & (merged_df['sentiment_label'] == 0), \"id\"]\n",
        "positive_only_df = merged_df.loc[(merged_df['sentiment_label'] == 1), \"id\"]\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "sarcasm_gerneral_negative = list(sarcasm_only_df.sample(n=30,random_state = 42))\n",
        "rest_negative = list(non_sarcasm_negative_only_df.sample(n=70, random_state = 42))\n",
        "sentiment_index = list(positive_only_df) + rest_negative + sarcasm_gerneral_negative\n",
        "\n",
        "rest = list(merged_df.loc[~merged_df['id'].isin(sentiment_index),\"id\"])\n",
        "\n",
        "sentiment = merged_df[merged_df['id'].isin(sentiment_index)]\n",
        "sarcasm = merged_df[~merged_df['id'].isin(sentiment_index)]"
      ],
      "metadata": {
        "id": "3R-V33U85GDF"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm.to_csv(\"sarcasm_dataset.csv\")\n",
        "sentiment.to_csv(\"sentiment_dataset.csv\")"
      ],
      "metadata": {
        "id": "3Nx0J1NafARs"
      },
      "execution_count": 172,
      "outputs": []
    }
  ]
}